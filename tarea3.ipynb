{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a6701ac",
   "metadata": {},
   "source": [
    "# Assignment: Convolutional Neural Networks\n",
    "\n",
    "## 1. Dataset Exploration (EDA)\n",
    "We will use the **CIFAR-10** dataset. It consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images.\n",
    "\n",
    "### Why CIFAR-10?\n",
    "- **Image-based**: 32x32 RGB images.\n",
    "- **Multi-class**: 10 distinct classes.\n",
    "- **Complexity**: Standard benchmark where CNNs significantly outperform dense networks, making it ideal for this comparison.\n",
    "- **Size**: Fits comfortably in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaec74f",
   "metadata": {},
   "source": [
    "### Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training images shape: {train_images.shape}\")\n",
    "print(f\"Test images shape: {test_images.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(train_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84283191",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Let's view 25 random images from the training set to understand the data diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i])\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b5d3a",
   "metadata": {},
   "source": [
    "## 2. Baseline Model (Non-Convolutional)\n",
    "We will implement a simple Multi-Layer Perceptron (MLP) consisting only of Dense (Fully Connected) layers.\n",
    "\n",
    "**Architecture:**\n",
    "- Flatten input (32x32x3 -> 3072)\n",
    "- Dense Layer (512 units, ReLU)\n",
    "- Dense Layer (256 units, ReLU)\n",
    "- Output Layer (10 units, Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80abf58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8878c2",
   "metadata": {},
   "source": [
    "### Training the Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e740b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "baseline_history = baseline_model.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d9670",
   "metadata": {},
   "source": [
    "## 3. Convolutional Architecture Design\n",
    "Now we design a CNN from scratch. \n",
    "\n",
    "**Architecture:**\n",
    "- **Conv2D**: 32 filters, 3x3 kernel. Extracts low-level features.\n",
    "- **MaxPooling2D**: 2x2. Reduces spatial dimensions.\n",
    "- **Conv2D**: 64 filters, 3x3 kernel. Extracts higher-level features.\n",
    "- **MaxPooling2D**: 2x2.\n",
    "- **Conv2D**: 64 filters, 3x3 kernel.\n",
    "- **Flatten**: Converts 3D volume to 1D vector.\n",
    "- **Dense**: 64 units, ReLU. Interpretation layer.\n",
    "- **Output**: 10 units. Classification.\n",
    "\n",
    "**Justification:**\n",
    "We use a stack of Conv2D and MaxPooling2D layers to progressively extract features and downsample the image. The number of filters increases (32 -> 64) as we go deeper to capture more complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e085c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = models.Sequential()\n",
    "cnn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Add Dense layers on top\n",
    "cnn_model.add(layers.Flatten())\n",
    "cnn_model.add(layers.Dense(64, activation='relu'))\n",
    "cnn_model.add(layers.Dense(10))\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506119c9",
   "metadata": {},
   "source": [
    "### Training the CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e144195",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn_history = cnn_model.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f4974",
   "metadata": {},
   "source": [
    "## Comparison of Results\n",
    "Let's compare the validation accuracy of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34621c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(baseline_history.history['val_accuracy'], label = 'Baseline (MLP)')\n",
    "plt.plot(cnn_history.history['val_accuracy'], label = 'CNN')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Val Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Baseline vs CNN Performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e1d3b",
   "metadata": {},
   "source": [
    "## 4. Controlled Experiments: Effect of Kernel Size\n",
    "\n",
    "We will investigate how changing the **Kernel Size** affects performance.\n",
    "- **Control**: Our previous CNN model (Kernel Size 3x3).\n",
    "- **Experiment**: A new CNN model with **Kernel Size 5x5** for the first two layers.\n",
    "\n",
    "**Hypothesis**: Larger kernels (5x5) have a wider receptive field but fewer parameters per unit of area coverage compared to stacked 3x3, though a single 5x5 has more parameters than a single 3x3. They might capture larger features earlier, but could miss fine-grained details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model = models.Sequential()\n",
    "# Changing kernel size to (5, 5)\n",
    "experiment_model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(32, 32, 3)))\n",
    "experiment_model.add(layers.MaxPooling2D((2, 2)))\n",
    "experiment_model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "experiment_model.add(layers.MaxPooling2D((2, 2)))\n",
    "# Third layer kept as 3x3 or removed if dimensions too small. Let's keep a small 3x3 here to match structure.\n",
    "experiment_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "experiment_model.add(layers.Flatten())\n",
    "experiment_model.add(layers.Dense(64, activation='relu'))\n",
    "experiment_model.add(layers.Dense(10))\n",
    "\n",
    "experiment_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126eeda",
   "metadata": {},
   "source": [
    "### Training the Experimental Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a089ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "experiment_history = experiment_model.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf336d5",
   "metadata": {},
   "source": [
    "### Experiment Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1de50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnn_history.history['val_accuracy'], label = 'CNN (3x3)')\n",
    "plt.plot(experiment_history.history['val_accuracy'], label = 'Experimental (5x5)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Val Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Effect of Kernel Size on Performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d777eb",
   "metadata": {},
   "source": [
    "## 5. Interpretation\n",
    "\n",
    "### Why did Convolutional layers outperform the Baseline?\n",
    "The CNN significantly outperforms the MLP Baseline because:\n",
    "1.  **Local Connectivity**: Convolutions exploit the local spatial correlation in images (nearby pixels are related).\n",
    "2.  **Parameter Sharing**: The same filter is applied across the entire image, drastically reducing the number of parameters compared to a Fully Connected layer where every input pixel has a unique weight.\n",
    "3.  **Translation Invariance**: Pooling and parameter sharing allow the network to recognize features regardless of their position in the image.\n",
    "\n",
    "### Effect of Kernel Size\n",
    "Comparing 3x3 vs 5x5 kernels helps us understand inductive bias. Typically, stacks of small kernels (3x3) are preferred in modern architectures (like VGG) because they introduce more non-linearities and have fewer parameters than a single large kernel with the same receptive field.\n",
    "\n",
    "### When is Convolution NOT appropriate?\n",
    "Convolution is not appropriate when **feature position is absolute and independent**, or when the data has **no spatial/temporal structure**. For example:\n",
    "- Tabular data (customer records, spreadsheets).\n",
    "- Permuted images (if we shuffled all pixels, CNNs would fail, but MLPs would perform the same)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
